{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 04-29 15:51:14 __init__.py:190] Automatically detected platform cuda.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from rouge_score import rouge_scorer\n",
    "import torch\n",
    "\n",
    "from transformers import AutoProcessor, AutoTokenizer\n",
    "from vllm import LLM, SamplingParams\n",
    "from qwen_vl_utils import process_vision_info\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BSZ = 64\n",
    "MODEL_PATH = \"/data/hf_cache/Qwen2.5-VL-7B-COT-SFT/\"\n",
    "file_name = \"trial_jupyter_notebook\"\n",
    "dataset_name = 'vsibench'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 04-29 15:51:25 config.py:542] This model supports multiple tasks: {'score', 'classify', 'generate', 'embed', 'reward'}. Defaulting to 'generate'.\n",
      "INFO 04-29 15:51:25 llm_engine.py:234] Initializing a V0 LLM engine (v0.7.2) with config: model='/data/hf_cache/Qwen2.5-VL-7B-COT-SFT/', speculative_config=None, tokenizer='/data/hf_cache/Qwen2.5-VL-7B-COT-SFT/', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=/data/hf_cache/Qwen2.5-VL-7B-COT-SFT/, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=False, \n",
      "INFO 04-29 15:51:27 cuda.py:230] Using Flash Attention backend.\n",
      "INFO 04-29 15:51:28 model_runner.py:1110] Starting to load model /data/hf_cache/Qwen2.5-VL-7B-COT-SFT/...\n",
      "INFO 04-29 15:51:28 config.py:2992] cudagraph sizes specified by model runner [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256] is overridden by config [256, 128, 2, 1, 4, 136, 8, 144, 16, 152, 24, 160, 32, 168, 40, 176, 48, 184, 56, 192, 64, 200, 72, 208, 80, 216, 88, 120, 224, 96, 232, 104, 240, 112, 248]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1fdda2f2e95e484fb3ceb0443d3fceee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 04-29 15:51:33 model_runner.py:1115] Loading model weights took 15.6270 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 04-29 15:51:37 worker.py:267] Memory profiling takes 4.46 seconds\n",
      "INFO 04-29 15:51:37 worker.py:267] the current vLLM instance can use total_gpu_memory (39.50GiB) x gpu_memory_utilization (0.80) = 31.60GiB\n",
      "INFO 04-29 15:51:37 worker.py:267] model weights take 15.63GiB; non_torch_memory takes 0.09GiB; PyTorch activation peak memory takes 1.25GiB; the rest of the memory reserved for KV Cache is 14.63GiB.\n",
      "INFO 04-29 15:51:38 executor_base.py:110] # CUDA blocks: 17119, # CPU blocks: 4681\n",
      "INFO 04-29 15:51:38 executor_base.py:115] Maximum concurrency for 8192 tokens per request: 33.44x\n",
      "INFO 04-29 15:51:43 model_runner.py:1434] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|████████████████████████████████████████████████████████████████████████████| 35/35 [00:20<00:00,  1.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 04-29 15:52:03 model_runner.py:1562] Graph capturing finished in 21 secs, took 0.37 GiB\n",
      "INFO 04-29 15:52:03 llm_engine.py:431] init engine (profile, create kv cache, warmup model) took 30.72 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "llm = LLM(\n",
    "    model=MODEL_PATH,\n",
    "    tensor_parallel_size=torch.cuda.device_count(),\n",
    "    max_model_len = 8192,\n",
    "    gpu_memory_utilization=0.8,\n",
    "    limit_mm_per_prompt={\"image\": 1, \"video\": 1},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_params = SamplingParams(\n",
    "    temperature=0.1,\n",
    "    top_p=0.001,\n",
    "    max_tokens=1024,\n",
    "    stop_token_ids=[],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = AutoProcessor.from_pretrained(MODEL_PATH)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
    "tokenizer.padding_side = \"left\"\n",
    "processor.tokenizer = tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_PATH = f\"./src/r1-v/eval_results/eval_{dataset_name}_{file_name}_greedy_output.json\"\n",
    "PROMPT_PATH = f\"./src/r1-v/Evaluation/eval_{dataset_name}.json\"\n",
    "\n",
    "if PROMPT_PATH.endswith('.jsonl'):\n",
    "    with open(PROMPT_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            data.append(json.loads(line))\n",
    "elif PROMPT_PATH.endswith('.json'):\n",
    "    with open(PROMPT_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "else:\n",
    "    raise ValueError(\"Input file must be .json or .jsonl\")\n",
    "\n",
    "QUESTION_TEMPLATE = (\n",
    "    \"{Question}\\n\"\n",
    "    \"Please think about this question as if you were a human pondering deeply. \"\n",
    "    \"Engage in an internal dialogue using expressions such as 'let me think', 'wait', 'Hmm', 'oh, I see', 'let's break it down', etc, or other natural language thought expressions \"\n",
    "    \"It's encouraged to include self-reflection or verification in the reasoning process. \"\n",
    "    \"Provide your detailed reasoning between the <think> and </think> tags, and then give your final answer between the <answer> and </answer> tags.\"\n",
    ")\n",
    "\n",
    "TYPE_TEMPLATE = {\n",
    "    \"multiple choice\": \" Please provide only the single option letter (e.g., A, B, C, D, etc.) within the <answer> </answer> tags.\",\n",
    "    \"numerical\": \" Please provide the numerical value (e.g., 42 or 3.14) within the <answer> </answer> tags.\",\n",
    "    \"OCR\": \" Please transcribe text from the image/video clearly and provide your text answer within the <answer> </answer> tags.\",\n",
    "    \"free-form\": \" Please provide your text answer within the <answer> </answer> tags.\",\n",
    "    \"regression\": \" Please provide the numerical value (e.g., 42 or 3.14) within the <answer> </answer> tags.\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "class StopOnSequence(transformers.StoppingCriteria):\n",
    "    def __init__(self, target_sequences, tokenizer):\n",
    "        # Encode the string so we have the exact token-IDs pattern\n",
    "        self.target_ids = [tokenizer.encode(target_sequence, add_special_tokens=False) for target_sequence in target_sequences]\n",
    "        self.target_lengths = [len(target_id) for target_id in self.target_ids]\n",
    "        self._tokenizer = tokenizer\n",
    "\n",
    "    def __call__(self, input_ids, scores, **kwargs):\n",
    "        # Make sure the target IDs are on the same device\n",
    "        targets = [torch.as_tensor(target_id, device=input_ids.device) for target_id in self.target_ids]\n",
    "\n",
    "        if input_ids.shape[1] < min(self.target_lengths):\n",
    "            return False\n",
    "\n",
    "        # Compare the tail of input_ids with our target_ids\n",
    "        for i, target in enumerate(targets):\n",
    "            if torch.equal(input_ids[0, -self.target_lengths[i]:], target):\n",
    "                return True\n",
    "\n",
    "        return False\n",
    "\n",
    "def extract_think(output_str):\n",
    "    pattern = r'<think>\\s*(.*?)\\s*</think>'\n",
    "    match = re.search(pattern, output_str, re.DOTALL)\n",
    "    if match:\n",
    "        return match.group(1).strip()\n",
    "    return \"\"\n",
    "\n",
    "def extract_answer(text):\n",
    "    pattern = r'<answer>\\s*(.*?)\\s*</answer>'\n",
    "    match = re.search(pattern, text, re.DOTALL)\n",
    "    if match:\n",
    "        return match.group(1).strip()\n",
    "    return \"\"\n",
    "\n",
    "def normalize_number(num_str):\n",
    "    try:\n",
    "        num_str = num_str.replace(',', '')\n",
    "        return float(num_str)\n",
    "    except Exception as e:\n",
    "        return None\n",
    "    \n",
    "def mean_relative_accuracy(pred, target, start=0.5, end=0.95, interval=0.05):\n",
    "\n",
    "    if not torch.is_tensor(pred):\n",
    "        pred = torch.tensor(pred, dtype=torch.float32)\n",
    "    if not torch.is_tensor(target):\n",
    "        target = torch.tensor(target, dtype=torch.float32)\n",
    "    \n",
    "    epsilon = 1e-8\n",
    "    rel_error = torch.abs(pred - target) / (torch.abs(target) + epsilon)\n",
    "    \n",
    "    thresholds = torch.arange(start, end + interval/2, interval, dtype=torch.float32)\n",
    "    \n",
    "    conditions = rel_error < (1 - thresholds)  \n",
    "    mra = conditions.float().mean()  \n",
    "    return mra.item()\n",
    "\n",
    "\n",
    "def reward_fn(sample, model_output, question_type):\n",
    "    try:\n",
    "        output_ans = extract_answer(model_output)\n",
    "        if output_ans == '':\n",
    "            output_ans = model_output\n",
    "        gt_ans = extract_answer(sample.get(\"solution\", \"\"))\n",
    "        if question_type == \"multiple choice\":\n",
    "            return 1.0 if output_ans.strip() == gt_ans.strip() else 0.0\n",
    "        elif question_type == \"numerical\":\n",
    "            gt_has_decimal = (\".\" in gt_ans) or (\",\" in gt_ans)\n",
    "            out_has_decimal = (\".\" in output_ans) or (\",\" in output_ans)\n",
    "            if gt_has_decimal != out_has_decimal:\n",
    "                return 0.0\n",
    "            gt_number = normalize_number(gt_ans)\n",
    "            out_number = normalize_number(output_ans)\n",
    "            if gt_number is None or out_number is None:\n",
    "                return 0.0\n",
    "            return 1.0 if round(gt_number, 2) == round(out_number, 2) else 0.0\n",
    "        elif question_type == \"regression\":\n",
    "            gt_number = normalize_number(gt_ans)\n",
    "            out_number = normalize_number(output_ans)\n",
    "            if gt_number is None or out_number is None:\n",
    "                return 0.0\n",
    "            mra = mean_relative_accuracy(out_number, gt_number)\n",
    "            return mra\n",
    "        else:\n",
    "            return 0.0\n",
    "    except Exception as e:\n",
    "        return 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = []\n",
    "for x in data:\n",
    "    if x[\"problem_type\"] == 'multiple choice':\n",
    "        question = x['problem'] + \"Options:\\n\"\n",
    "        for op in x[\"options\"]:\n",
    "            question += op + \"\\n\"\n",
    "    else:\n",
    "        question = x['problem']\n",
    "\n",
    "    msg = [{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\n",
    "                \"type\": x['data_type'],\n",
    "                x['data_type']: os.getcwd() + \"/src/r1-v\" + x['path'][1:]\n",
    "            },\n",
    "            {\n",
    "                \"type\": \"text\",\n",
    "                \"text\": QUESTION_TEMPLATE.format(Question=question) + TYPE_TEMPLATE[x['problem_type']]\n",
    "            }\n",
    "        ]\n",
    "    }]\n",
    "    messages.append(msg)\n",
    "    \n",
    "\n",
    "final_output = []\n",
    "start_idx = 0\n",
    "if os.path.exists(OUTPUT_PATH):\n",
    "    try:\n",
    "        with open(OUTPUT_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "            existing = json.load(f)\n",
    "            final_output = existing.get(\"results\", [])\n",
    "            start_idx = len(final_output)\n",
    "            print(f\"Resuming from sample index {start_idx}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading existing output file: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   0%|                                                                                              | 0/81 [00:00<?, ?it/s]qwen-vl-utils using decord to read video.\n",
      "\n",
      "Processed prompts:   0%|                                           | 0/64 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\n",
      "Processed prompts:   2%|▌                                 | 1/64 [00:13<13:57, 13.29s/it, est. speed input: 90.57 toks/s, output: 8.80 toks/s]\u001b[A\n",
      "Processed prompts:   5%|█▌                              | 3/64 [00:13<03:32,  3.49s/it, est. speed input: 269.53 toks/s, output: 26.64 toks/s]\u001b[A\n",
      "Processed prompts:   8%|██▌                             | 5/64 [00:13<01:42,  1.74s/it, est. speed input: 443.10 toks/s, output: 45.12 toks/s]\u001b[A\n",
      "Processed prompts:  11%|███▌                            | 7/64 [00:13<00:58,  1.03s/it, est. speed input: 615.59 toks/s, output: 63.84 toks/s]\u001b[A\n",
      "Processed prompts:  17%|█████▏                        | 11/64 [00:13<00:25,  2.08it/s, est. speed input: 958.42 toks/s, output: 102.18 toks/s]\u001b[A\n",
      "Processed prompts:  22%|██████▎                      | 14/64 [00:13<00:16,  3.11it/s, est. speed input: 1207.06 toks/s, output: 131.40 toks/s]\u001b[A\n",
      "Processed prompts:  33%|█████████▌                   | 21/64 [00:14<00:06,  6.52it/s, est. speed input: 1795.33 toks/s, output: 202.51 toks/s]\u001b[A\n",
      "Processed prompts:  39%|███████████▎                 | 25/64 [00:14<00:04,  8.58it/s, est. speed input: 2116.57 toks/s, output: 243.09 toks/s]\u001b[A\n",
      "Processed prompts:  45%|█████████████▏               | 29/64 [00:14<00:03, 11.15it/s, est. speed input: 2435.26 toks/s, output: 285.19 toks/s]\u001b[A\n",
      "Processed prompts:  52%|██████████████▉              | 33/64 [00:14<00:02, 13.84it/s, est. speed input: 2746.13 toks/s, output: 327.47 toks/s]\u001b[A\n",
      "Processed prompts:  58%|████████████████▊            | 37/64 [00:14<00:01, 15.48it/s, est. speed input: 3039.53 toks/s, output: 369.95 toks/s]\u001b[A\n",
      "Processed prompts:  66%|███████████████████          | 42/64 [00:14<00:01, 20.41it/s, est. speed input: 3426.57 toks/s, output: 427.03 toks/s]\u001b[A\n",
      "Processed prompts:  72%|████████████████████▊        | 46/64 [00:14<00:00, 23.12it/s, est. speed input: 3723.69 toks/s, output: 472.86 toks/s]\u001b[A\n",
      "Processed prompts:  81%|███████████████████████▌     | 52/64 [00:15<00:00, 26.09it/s, est. speed input: 4158.58 toks/s, output: 542.61 toks/s]\u001b[A\n",
      "Processed prompts:  88%|█████████████████████████▍   | 56/64 [00:15<00:00, 23.91it/s, est. speed input: 4418.02 toks/s, output: 588.03 toks/s]\u001b[A\n",
      "Processed prompts:  92%|██████████████████████████▋  | 59/64 [00:15<00:00, 19.34it/s, est. speed input: 4576.97 toks/s, output: 620.80 toks/s]\u001b[A\n",
      "Processed prompts:  97%|████████████████████████████ | 62/64 [00:16<00:00,  8.40it/s, est. speed input: 4519.85 toks/s, output: 632.01 toks/s]\u001b[A\n",
      "Processed prompts: 100%|█████████████████████████████| 64/64 [00:27<00:00,  2.33it/s, est. speed input: 2808.18 toks/s, output: 455.03 toks/s]\u001b[A\n",
      "Processing batches:   1%|█                                                                                  | 1/81 [02:15<3:00:27, 135.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed batch 1, saved 64 samples.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed prompts:   0%|                                           | 0/64 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\n",
      "Processed prompts:   2%|▌                                 | 1/64 [00:12<13:36, 12.96s/it, est. speed input: 92.90 toks/s, output: 8.26 toks/s]\u001b[A\n",
      "Processed prompts:   3%|█                               | 2/64 [00:13<05:36,  5.43s/it, est. speed input: 183.52 toks/s, output: 16.77 toks/s]\u001b[A\n",
      "Processed prompts:   5%|█▌                              | 3/64 [00:13<03:05,  3.04s/it, est. speed input: 271.43 toks/s, output: 25.55 toks/s]\u001b[A\n",
      "Processed prompts:   8%|██▌                             | 5/64 [00:13<01:19,  1.35s/it, est. speed input: 448.80 toks/s, output: 43.61 toks/s]\u001b[A\n",
      "Processed prompts:  11%|███▌                            | 7/64 [00:13<00:44,  1.27it/s, est. speed input: 619.84 toks/s, output: 62.22 toks/s]\u001b[A\n",
      "Processed prompts:  22%|██████▎                      | 14/64 [00:13<00:12,  3.96it/s, est. speed input: 1230.62 toks/s, output: 129.81 toks/s]\u001b[A\n",
      "Processed prompts:  31%|█████████                    | 20/64 [00:13<00:06,  6.85it/s, est. speed input: 1743.12 toks/s, output: 188.21 toks/s]\u001b[A\n",
      "Processed prompts:  39%|███████████▎                 | 25/64 [00:13<00:04,  9.73it/s, est. speed input: 2160.33 toks/s, output: 238.21 toks/s]\u001b[A\n",
      "Processed prompts:  45%|█████████████▏               | 29/64 [00:14<00:02, 12.27it/s, est. speed input: 2485.42 toks/s, output: 278.40 toks/s]\u001b[A\n",
      "Processed prompts:  55%|███████████████▊             | 35/64 [00:14<00:01, 17.39it/s, est. speed input: 2976.12 toks/s, output: 341.05 toks/s]\u001b[A\n",
      "Processed prompts:  62%|██████████████████▏          | 40/64 [00:14<00:01, 18.80it/s, est. speed input: 3349.56 toks/s, output: 392.20 toks/s]\u001b[A\n",
      "Processed prompts:  70%|████████████████████▍        | 45/64 [00:14<00:00, 22.70it/s, est. speed input: 3737.63 toks/s, output: 447.78 toks/s]\u001b[A\n",
      "Processed prompts:  77%|██████████████████████▏      | 49/64 [00:14<00:00, 25.03it/s, est. speed input: 4038.36 toks/s, output: 492.64 toks/s]\u001b[A\n",
      "Processed prompts:  84%|████████████████████████▍    | 54/64 [00:14<00:00, 26.10it/s, est. speed input: 4398.01 toks/s, output: 548.33 toks/s]\u001b[A\n",
      "Processed prompts:  91%|██████████████████████████▎  | 58/64 [00:15<00:00, 16.11it/s, est. speed input: 4566.24 toks/s, output: 583.01 toks/s]\u001b[A\n",
      "Processed prompts:  95%|███████████████████████████▋ | 61/64 [00:15<00:00, 11.83it/s, est. speed input: 4653.38 toks/s, output: 610.28 toks/s]\u001b[A\n",
      "Processed prompts:  98%|████████████████████████████▌| 63/64 [00:26<00:00, 11.83it/s, est. speed input: 4703.14 toks/s, output: 630.89 toks/s]\u001b[A\n",
      "Processed prompts: 100%|█████████████████████████████| 64/64 [00:27<00:00,  2.36it/s, est. speed input: 2841.73 toks/s, output: 413.01 toks/s]\u001b[A\n",
      "Processing batches:   2%|██                                                                                 | 2/81 [04:20<2:49:57, 129.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed batch 2, saved 128 samples.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   2%|██                                                                                 | 2/81 [05:32<3:39:04, 166.39s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      6\u001b[39m prompts = [processor.apply_chat_template(msg, tokenize=\u001b[38;5;28;01mFalse\u001b[39;00m, add_generation_prompt=\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;28;01mfor\u001b[39;00m msg \u001b[38;5;129;01min\u001b[39;00m batch_messages]\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m     image_inputs, video_inputs, video_kwargs = \u001b[43mprocess_vision_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_messages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_video_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     12\u001b[39m     image_idx = \u001b[32m0\u001b[39m\n\u001b[32m     13\u001b[39m     video_idx = \u001b[32m0\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/work/active_perception_CoT/Video-R1/src/qwen-vl-utils/src/qwen_vl_utils/vision_process.py:368\u001b[39m, in \u001b[36mprocess_vision_info\u001b[39m\u001b[34m(conversations, return_video_kwargs)\u001b[39m\n\u001b[32m    366\u001b[39m     image_inputs.append(fetch_image(vision_info))\n\u001b[32m    367\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mvideo\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m vision_info:\n\u001b[32m--> \u001b[39m\u001b[32m368\u001b[39m     video_input, video_sample_fps = \u001b[43mfetch_video\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvision_info\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_video_sample_fps\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    369\u001b[39m     video_sample_fps_list.append(video_sample_fps)\n\u001b[32m    370\u001b[39m     video_inputs.append(video_input)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/work/active_perception_CoT/Video-R1/src/qwen-vl-utils/src/qwen_vl_utils/vision_process.py:310\u001b[39m, in \u001b[36mfetch_video\u001b[39m\u001b[34m(ele, image_factor, return_video_sample_fps)\u001b[39m\n\u001b[32m    302\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    303\u001b[39m     resized_height, resized_width = smart_resize(\n\u001b[32m    304\u001b[39m         height,\n\u001b[32m    305\u001b[39m         width,\n\u001b[32m   (...)\u001b[39m\u001b[32m    308\u001b[39m         max_pixels=max_pixels,\n\u001b[32m    309\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m310\u001b[39m video = \u001b[43mtransforms\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfunctional\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    311\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvideo\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    312\u001b[39m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[43mresized_height\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresized_width\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    313\u001b[39m \u001b[43m    \u001b[49m\u001b[43minterpolation\u001b[49m\u001b[43m=\u001b[49m\u001b[43mInterpolationMode\u001b[49m\u001b[43m.\u001b[49m\u001b[43mBICUBIC\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    314\u001b[39m \u001b[43m    \u001b[49m\u001b[43mantialias\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    315\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m.float()\n\u001b[32m    316\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m return_video_sample_fps:\n\u001b[32m    317\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m video, sample_fps\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/video-cot/lib/python3.11/site-packages/torchvision/transforms/functional.py:479\u001b[39m, in \u001b[36mresize\u001b[39m\u001b[34m(img, size, interpolation, max_size, antialias)\u001b[39m\n\u001b[32m    476\u001b[39m     pil_interpolation = pil_modes_mapping[interpolation]\n\u001b[32m    477\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m F_pil.resize(img, size=output_size, interpolation=pil_interpolation)\n\u001b[32m--> \u001b[39m\u001b[32m479\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF_t\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minterpolation\u001b[49m\u001b[43m=\u001b[49m\u001b[43minterpolation\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mantialias\u001b[49m\u001b[43m=\u001b[49m\u001b[43mantialias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/video-cot/lib/python3.11/site-packages/torchvision/transforms/_functional_tensor.py:467\u001b[39m, in \u001b[36mresize\u001b[39m\u001b[34m(img, size, interpolation, antialias)\u001b[39m\n\u001b[32m    464\u001b[39m \u001b[38;5;66;03m# Define align_corners to avoid warnings\u001b[39;00m\n\u001b[32m    465\u001b[39m align_corners = \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m interpolation \u001b[38;5;129;01min\u001b[39;00m [\u001b[33m\"\u001b[39m\u001b[33mbilinear\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mbicubic\u001b[39m\u001b[33m\"\u001b[39m] \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m467\u001b[39m img = \u001b[43minterpolate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[43m=\u001b[49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m=\u001b[49m\u001b[43minterpolation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malign_corners\u001b[49m\u001b[43m=\u001b[49m\u001b[43malign_corners\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mantialias\u001b[49m\u001b[43m=\u001b[49m\u001b[43mantialias\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    469\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m interpolation == \u001b[33m\"\u001b[39m\u001b[33mbicubic\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m out_dtype == torch.uint8:\n\u001b[32m    470\u001b[39m     img = img.clamp(\u001b[38;5;28mmin\u001b[39m=\u001b[32m0\u001b[39m, \u001b[38;5;28mmax\u001b[39m=\u001b[32m255\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/video-cot/lib/python3.11/site-packages/torch/nn/functional.py:4591\u001b[39m, in \u001b[36minterpolate\u001b[39m\u001b[34m(input, size, scale_factor, mode, align_corners, recompute_scale_factor, antialias)\u001b[39m\n\u001b[32m   4589\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m align_corners \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   4590\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m antialias:\n\u001b[32m-> \u001b[39m\u001b[32m4591\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_C\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_nn\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_upsample_bicubic2d_aa\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4592\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malign_corners\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_factors\u001b[49m\n\u001b[32m   4593\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4594\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m torch._C._nn.upsample_bicubic2d(\n\u001b[32m   4595\u001b[39m         \u001b[38;5;28minput\u001b[39m, output_size, align_corners, scale_factors\n\u001b[32m   4596\u001b[39m     )\n\u001b[32m   4598\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28minput\u001b[39m.dim() == \u001b[32m3\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m mode == \u001b[33m\"\u001b[39m\u001b[33mbilinear\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "mean_acc = []\n",
    "mean_mra = []\n",
    "for i in tqdm(range(start_idx, len(messages), BSZ), desc=\"Processing batches\"):\n",
    "    batch_messages = messages[i:i + BSZ]\n",
    "\n",
    "    prompts = [processor.apply_chat_template(msg, tokenize=False, add_generation_prompt=True) for msg in batch_messages]\n",
    "    \n",
    "\n",
    "    try:\n",
    "        image_inputs, video_inputs, video_kwargs = process_vision_info(batch_messages, return_video_kwargs=True)\n",
    "        \n",
    "        image_idx = 0\n",
    "        video_idx = 0\n",
    "\n",
    "        llm_inputs = []\n",
    "\n",
    "        \n",
    "        for idx, prompt in enumerate(prompts):\n",
    "            mm_type = batch_messages[idx][0]['content'][0]['type']\n",
    "            sample_mm_data = {}\n",
    "            sample_video_kw = {}\n",
    "            if mm_type == 'image':\n",
    "                sample_mm_data[\"image\"] = image_inputs[image_idx]\n",
    "                image_idx += 1\n",
    "            elif mm_type == 'video':\n",
    "                sample_mm_data[\"video\"] = video_inputs[video_idx]\n",
    "                for key, value in video_kwargs.items():\n",
    "                    sample_video_kw[key] = value[video_idx]\n",
    "                video_idx += 1\n",
    "                    \n",
    "            \n",
    "            llm_inputs.append({\n",
    "                \"prompt\": prompt,\n",
    "                \"multi_modal_data\": sample_mm_data,\n",
    "                \"mm_processor_kwargs\": sample_video_kw,\n",
    "            })\n",
    "            \n",
    "\n",
    "        outputs = llm.generate(llm_inputs, sampling_params=sampling_params)\n",
    "        batch_output_text = [out.outputs[0].text for out in outputs]\n",
    "        \n",
    "    except Exception as e:\n",
    "        print('error:', data[i]['path'])\n",
    "        batch_output_text = ['<answer>error</answer>'] * BSZ\n",
    "        \n",
    "\n",
    "    for j, (sample, model_output) in enumerate(zip(data[i:i+BSZ], batch_output_text), start=i):\n",
    "        think_chain = extract_think(model_output)\n",
    "        final_ans = extract_answer(model_output)\n",
    "        if final_ans == \"\":\n",
    "            final_ans = model_output\n",
    "        sample[\"output\"] = model_output\n",
    "        sample[\"prediction\"] = final_ans\n",
    "        q_type = sample.get(\"problem_type\", \"\")\n",
    "        sample[\"reward\"] = reward_fn(sample, model_output, q_type)\n",
    "        sample['correct'] = True if sample[\"reward\"]==1.0 else False\n",
    "        if sample['problem_type'] != 'regression':\n",
    "            mean_acc.append(sample[\"reward\"])\n",
    "        else:\n",
    "            mean_mra.append(sample[\"reward\"])\n",
    "        if think_chain:\n",
    "            sample[\"process\"] = f\"<think>{think_chain}</think>\"\n",
    "        final_output.append(sample)\n",
    "    \n",
    "\n",
    "    try:\n",
    "        with open(OUTPUT_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump({\"results\": final_output}, f, indent=2, ensure_ascii=False)\n",
    "        print(f\"Processed batch {(i - start_idx)//BSZ + 1}, saved {len(final_output)} samples.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error writing to output file: {e}\")\n",
    "\n",
    "final_acc={'mean_acc': 0.0, 'mean_mra': 0.0}\n",
    "final_acc['mean_acc'] = torch.tensor(mean_acc).mean().item()\n",
    "if mean_mra != []:\n",
    "    final_acc['mean_mra'] = torch.tensor(mean_mra).mean().item()\n",
    "\n",
    "try:\n",
    "    with open(OUTPUT_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump({\"results\": final_output, \"final_acc\": [final_acc]}, f, indent=2, ensure_ascii=False)\n",
    "    print(f\"Final accuracy saved to {OUTPUT_PATH}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error writing final accuracy to output file: {e}\")\n",
    "\n",
    "print(f\"Results saved to {OUTPUT_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
